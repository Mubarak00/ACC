{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7ec591f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NET_PC\\.conda\\envs\\Mubarak\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\NET_PC\\.conda\\envs\\Mubarak\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.10.0 and strictly below 2.13.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.9.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from vit_keras import vit, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0bbf408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NET_PC\\.conda\\envs\\Mubarak\\lib\\site-packages\\vit_keras\\utils.py:81: UserWarning: Resizing position embeddings from 24, 24 to 4, 4\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define input shape\n",
    "input_shape = (64, 64, 3)\n",
    "\n",
    "# Define number of classes\n",
    "num_classes = 1\n",
    "\n",
    "# Load ViT model\n",
    "vit_model = vit.vit_b16(\n",
    "    image_size=64,\n",
    "    activation='sigmoid',\n",
    "    pretrained=True,\n",
    "    include_top=False,\n",
    "    pretrained_top=False,\n",
    "    classes=num_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "112d737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To suppress the warning message\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='Resizing position embeddings from 24, 24 to 4, 4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "530e664c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers except the last few\n",
    "for layer in vit_model.layers[:-4]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb2ac8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom layers for classification\n",
    "flatten_layer = Flatten()(vit_model.output)\n",
    "dense_layer = Dense(2048, activation='relu')(flatten_layer)\n",
    "dense_layer = Dense(1024, activation='relu')(flatten_layer)\n",
    "dense_layer = Dense(512, activation='relu')(flatten_layer)\n",
    "dense_layer = Dense(128, activation='relu')(flatten_layer)\n",
    "dense_layer = Dense(64, activation='relu')(flatten_layer)\n",
    "dense_layer = Dense(32, activation='relu')(flatten_layer)\n",
    "output_layer = Dense(num_classes, activation='sigmoid')(dense_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b39488a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Model(inputs=vit_model.input, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7adbd556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 64, 64, 3)]       0         \n",
      "                                                                 \n",
      " embedding (Conv2D)          (None, 4, 4, 768)         590592    \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 16, 768)           0         \n",
      "                                                                 \n",
      " class_token (ClassToken)    (None, 17, 768)           768       \n",
      "                                                                 \n",
      " Transformer/posembed_input   (None, 17, 768)          13056     \n",
      " (AddPositionEmbs)                                               \n",
      "                                                                 \n",
      " Transformer/encoderblock_0   ((None, 17, 768),        7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_1   ((None, 17, 768),        7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_2   ((None, 17, 768),        7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_3   ((None, 17, 768),        7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_4   ((None, 17, 768),        7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_5   ((None, 17, 768),        7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_6   ((None, 17, 768),        7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_7   ((None, 17, 768),        7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_8   ((None, 17, 768),        7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_9   ((None, 17, 768),        7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_10  ((None, 17, 768),        7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_11  ((None, 17, 768),        7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoder_norm (L  (None, 17, 768)          1536      \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " ExtractToken (Lambda)       (None, 768)               0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 768)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 32)                24608     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 85,685,057\n",
      "Trainable params: 14,201,921\n",
      "Non-trainable params: 71,483,136\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c70acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "image_dir = Path(\"C:/Users/NET_PC/Desktop/Adrenocortical carcionoma project/JPG 2 Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c46960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pandas as pd\n",
    "\n",
    "filepaths = list(image_dir.glob(r'**/*.JPG'))\n",
    "labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n",
    "\n",
    "filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
    "labels = pd.Series(labels, name='Label')\n",
    "\n",
    "image_df = pd.concat([filepaths, labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f407da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23316 validated image filenames belonging to 2 classes.\n",
      "Found 5828 validated image filenames belonging to 2 classes.\n",
      "Found 7286 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "image_df\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(image_df, train_size=0.80, shuffle=True, random_state=1)\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255,\n",
    "                                                                  validation_split=0.2)\n",
    "\n",
    "test_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_images = train_generator.flow_from_dataframe(dataframe=train_df,\n",
    "                                                   x_col='Filepath',\n",
    "                                                   y_col='Label',\n",
    "                                                   target_size=(64, 64),\n",
    "                                                   color_mode='rgb',\n",
    "                                                   class_mode='binary',\n",
    "                                                   batch_size=32,\n",
    "                                                   shuffle=True,\n",
    "                                                   seed=42,\n",
    "                                                   subset='training')\n",
    "\n",
    "val_images = train_generator.flow_from_dataframe(dataframe=train_df,\n",
    "                                                 x_col='Filepath',\n",
    "                                                 y_col='Label',\n",
    "                                                 target_size=(64, 64),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 class_mode='binary',\n",
    "                                                 batch_size=32,\n",
    "                                                 shuffle=True,\n",
    "                                                 seed=42,\n",
    "                                                 subset='validation')\n",
    "\n",
    "test_images = test_generator.flow_from_dataframe(dataframe=test_df,\n",
    "                                                 x_col='Filepath',\n",
    "                                                 y_col='Label',\n",
    "                                                 target_size=(64, 64),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 class_mode='binary',\n",
    "                                                 batch_size=32,\n",
    "                                                 shuffle=False)\n",
    "\n",
    "history = model.fit(\n",
    "    train_images,\n",
    "    validation_data=val_images,\n",
    "    epochs=50,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            patience=3\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585ecda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test_images, verbose=0)\n",
    "\n",
    "print(\"    Test Loss: {:.5f}\".format(results[0]))\n",
    "print(\"Test Accuracy: {:.2f}%\".format(results[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c10286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "\n",
    "plt.title('ViT Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Accuracy', 'Validation Accuracy', 'loss', 'Validation Loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beac89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('ViT Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Loss', 'Validation Loss'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d784e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "predictions = (model.predict(test_images) >= 0.5).astype(int)\n",
    "\n",
    "cm = confusion_matrix(test_images.labels, predictions, labels=[0, 1])\n",
    "clr = classification_report(test_images.labels, predictions, labels=[0, 1], target_names=[\"ACC\", \"Kidney tumor\"])\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='g', vmin=0, cmap='Blues', cbar=False)\n",
    "plt.xticks(ticks=[0.5, 1.5], labels=[\"ACC\", \"Kidney tumor\"])\n",
    "plt.yticks(ticks=[0.5, 1.5], labels=[\"ACC\", \"Kidney tumor\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix of ViT Model\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Classification Report:\\n----------------------\\n\", clr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f613a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain predicted probabilities\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "predicted_probabilities = model.predict(test_images)\n",
    "\n",
    "# Compute AUC ROC\n",
    "auc_roc = roc_auc_score(test_images.labels, predicted_probabilities)\n",
    "\n",
    "print(\"AUC ROC: {:.5f}\".format(auc_roc))\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(test_images.labels, predicted_probabilities)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = {:.2f})'.format(auc_roc))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de400e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to a file\n",
    "model.save('ViT2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37353eed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
